{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>Comperative Analysis of k-Nearest-Neighbour and Softmax Regression on MNIST Dataset </h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1DXCGDC6JWx"
   },
   "source": [
    "# Abstract\n",
    "This study conducts a comparative analysis of Softmax Regression (Multinomial Logistic Regression) and k-Nearest Neighbors (kNN) in the realm of multi-class classification, with both algorithms implemented from scratch and benchmarked against their Scikit-learn equivalents. Softmax Regression, a linear classifier, and kNN, a non-parametric method, were evaluated on their ability to accurately classify digit images to the well known MNIST dataset of handwritten digits.\n",
    "\n",
    "The study demonstrates the strengths and limitations of each algorithm. Softmax Regression shown to be \n",
    "more scalable and offered valuable insights through its interpretable coefficients. On the other hand, kNN showed a slightly higher accuracy and F1 score, though at the cost of computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2idGbM-t6JW0"
   },
   "source": [
    "# Table of Contents\n",
    "- [Abstract](#abstract)\n",
    "- [1. Introduction](#introduction)\n",
    "  - [1.1 Aims and Objectives](#introduction)\n",
    "  - [1.2 Dataset](#introduction)\n",
    "- [2. Background](#background)\n",
    "  - [2.1 Softmax Regression (Multinomial Logistic Regression)](#softmaxRegression)\n",
    "    - [2.1.1 Methodology of Softmax Regression](#methodologySoftmaxRegression)\n",
    "    - [2.1.2 Linear Combination](#linearCombination)\n",
    "    - [2.1.3 Softmax Function](#softmaxFunction)\n",
    "    - [2.1.4 Training in Softmax Regression](#trainingSoftmax)\n",
    "      - [I. One-Hot Encoding](#oneHotEncoding)\n",
    "      - [II. Batch Gradient Descent](#gradientDescent)\n",
    "      - [III. Loss Function](#lossFunction)\n",
    "    - [2.1.5 Prediction in Softmax Regression](#predictionSoftmaxRegression)\n",
    "  - [2.2 k-Nearest Neighbours](#kNN)\n",
    "- [3. Methodology](#methodology)\n",
    "- [4. Results](#results)\n",
    "  - [4.1 Softmax Regression Loss Function](#softmaxresults)\n",
    "  - [4.2 Confusion Matrices](#confusionresults)\n",
    "  - [4.3 3-Fold Cross Validation](#foldresults)\n",
    "  - [4.4 Softmax Regression Decision Process](#decisionresults)\n",
    "- [5. Evaluation](#evaluation)\n",
    "- [6. Conclusion](#conclusion)\n",
    "- [7. Future Work](#futurework)\n",
    "- [8. References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POrd6PoI6JW1"
   },
   "source": [
    "# 1. Introduction <a id=\"introduction\"></a>\n",
    "Pattern recognition involves teaching machines to identify different patterns, features, or relationships within data. One of the key challenges in this area is multi class classification where a machine learns to recognize and categorize data into more than just two groups. This type of classification is interesting because it's more complex and closer to how we, as humans, see and understand the world.\n",
    "\n",
    "## 1.1 Aims and Objectives <a id=\"aims\"></a>\n",
    "In this study, we will be conducting a comparative analysis of two machine learning models, the parametric Softmax Regression (Multinomial Logistic Regression) and the non-parametric k-Nearest Neighbors (kNN) within the context of multi class classification. The comparison is interesting because it compares a linear model that relies on statistical assumptions about the data with an assumption free model.\n",
    "\n",
    "The aim of this study is to showcase the strengths and limitations of these models when applied to a standard benchmark in classification using the MNIST dataset of handwritten digits. with a focus on implementing Softmax Regression and kNN from scratch and comparing their performance with their respective Scikit-learn implementations on this dataset. And to find which model is better suited for multi class classification.\n",
    "\n",
    "## 1.2 Dataset <a id=\"dataset\"></a>\n",
    "The objective of this dataset is to classify grayscaled images of handwritten digits into ten labels (from 0 to 9). The following figure shows a few examples of images from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy2ox8mR6JW1"
   },
   "source": [
    "![](images/digits_grey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsT37cGZ6JW1"
   },
   "source": [
    "The plot above displays few images and their labels for each digit. The dataset contains 70,000 labeled images which is large enough to test our classifier models with balanced number of images for each digit, as shown in the following chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI-AJ_Mc6JW1"
   },
   "source": [
    "![](images/class_distrubution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS2gtS3_6JW1"
   },
   "source": [
    "This chart displays the frequency of each digits in the dataset. The distrubtion of the digits is well balanced.\n",
    "\n",
    "The dataset also presents challenges such as distorted digits with irregular shapes, incomplete strokes, and varying skew in both the training and testing [1]. We will provide a deeper insight into the inner workings of Softmax Regression and k-Nearest Neighbors (kNN) in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAQoCovi6JW1"
   },
   "source": [
    "# 2. Background <a id=\"background\"></a>\n",
    "In this section, we will explore the theoretical underpinnings and the mechanisms of both Softmax Regression and k-Nearest Neighbors (kNN) for our study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-W0yxMr6JW1"
   },
   "source": [
    "## 2.1 Softmax Regression (Multinomial Logistic Regression) <a class=\"anchor\" id=\"softmaxRegression\"></a>\n",
    "Softmax Regression, which is also known as Multinomial Logistic Regression [2].  is a linear, parametric, supervised machine learning model used for classification of an input into multiple classes, unlike logistic regression which can only classify an input into two classes (binary).\n",
    "\n",
    "Logistic Regression uses a sigmoid function to convert the logit (linear output) into probability for two class classification. Softmax Regression Extended this idea by using the softmax function, which is a generalization of sigmoid function to convert logits (linear outputs) into probability distrubtion for multiple classes classification, and the class with the highest probability is then predicted using argmax function. Hence why it's called Multinomial Logistic Regression and Softmax Regression.\n",
    "\n",
    "The approach to understanding and implementing Softmax Regression was inspired by the Wikipedia page on Multinomial Logistic Regression [2], and from the book 'The Elements of Statistical Learning' [3] in section 4.4 about Logistic Regression. We will delve into details on the how the Softmax Regression Classification works. \n",
    "\n",
    "### 2.1.1 Methodology of Softmax Regression <a class=\"anchor\" id=\"methodologySoftmaxRegression\"></a>\n",
    "In this section, we will delve into how the softmax regression algorithmically works. Here is a brief overview of the steps for training phase:\n",
    "1. Given a data point, compute the linear combination of weights and inputs.\n",
    "2. Apply the softmax function to get probabilities for each class.\n",
    "3. Compute the error using the cross-entropy loss function.\n",
    "4. Use gradient descent to update weights and bias.\n",
    "5. Repeat for a specified number of iterations which is also known as `epoch`.\n",
    "\n",
    "And the steps for the prediction phase:\n",
    "\n",
    "1. For a new data point, compute the linear combination of weights and inputs.\n",
    "2. Apply the softmax function to get probabilities for each class.\n",
    "3. Predict the class with the highest probability.\n",
    "\n",
    "In the following sections, we will delve into the details for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy1ANLAp6JW2"
   },
   "source": [
    "### 2.1.2 Linear Combination <a id=\"linearCombination\"></a>\n",
    "Given a data point, we compute the linear combinations by multiplying each input feature with its corresponding weight and then results are summed up with the bias.\n",
    "\n",
    "$$\n",
    "Z_i = \\mathbf{W}_i^\\top \\mathbf{X} + b_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Z_i $ is the linear combination for class $ i $.\n",
    "- $ \\mathbf{W}_i $ is the weight matrix for class.\n",
    "- $ \\mathbf{X} $ is the input feature vector\n",
    "- $ b_i $ is the bias term for class $ i $.\n",
    "\n",
    "\n",
    "This give us the logit/score for each class. Which we then use it as an input for the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emaJOMlT6JW2"
   },
   "source": [
    "### 2.1.3 Softmax Function <a id=\"softmaxFunction\"></a>\n",
    "We pass the computed linear combinations as an input to the softmax function, so that we convert these logits/scores into probabilities. The softmax function is represented as following:\n",
    "\n",
    "$$\n",
    "P(y = i | \\mathbf{X}) = \\frac{e^{Z_i}}{\\sum_{j=1}^{K} e^{Z_j}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ P(y = i | \\mathbf{X}) $ is the probability that the input $ \\mathbf{X} $ that belongs to class $ i $.\n",
    "- $ Z_i $ is the linear combination for class $ i $.\n",
    "- The sum in the denominator $ {\\sum_{j=1}^{K} e^{Z_j}} $  is taken over all classes $ j $.\n",
    "\n",
    "The softmax function takes the exponential of each linear combination $ e^{Z_i} $ for each class to transform into a positive value. Each exponentiated value is divided by $ {\\sum_{j=1}^{K} e^{Z_j}} $ of exponentiated values for all classes from 1 to $ {K} $, so that the output is normalized into the sum of all classes equal to 1, making it a valid probability distrubtion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UadO3Jmq6JW2"
   },
   "source": [
    "### 2.1.4 Training in Softmax Regression <a id=\"trainingSoftmaxRegression\"></a>\n",
    "Training softmax regression is about minimizing the loss function that captures the difference between predicted probabilities and the actual class labels using Gradient.\n",
    "\n",
    "#### I. One-Hot Encoding <a id=\"oneHotEncoding\"></a>\n",
    "Before training, the target variable y is transformed into a one hot encoded format. This encoding is a method that we use to represent categorical labels such as (Cat, Dog, Goat) into binary matrices for our model.\n",
    "\n",
    "This encoding is essential for multi class classification like ours where the model outputs a probability distrubution, since it helps us make clear distincition between classes in the computation of the loss function.\n",
    "\n",
    "#### II. Batch Gradient Descent <a id=\"gradientDescent\"></a>\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that is core of our training process that we use to minimize the cost function (cross-entropy loss). We use Batch Gradient Descent algorithm that works by computing the gradients for weights and bias by comparing the model's probability output with the one hot encoded classes. The model paramters are then updated in the direction that minimze the loss function.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\mathbf{W} - \\alpha \\cdot \\nabla_{\\mathbf{W}} J\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\nabla_{b} J\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{W} $ are the weights.\n",
    "- $ b $ is the bias.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\nabla_{\\mathbf{W}} J $ and $ \\nabla_{b} J $ are the gradients of the cost function $ J $ with respect to the weights and bias.\n",
    "\n",
    "The gradients $ \\nabla_{\\mathbf{W}} J $ and $ \\nabla_{b} J $ gives us the direction in which the loss function is increasing the most. The algorithm goal is to reduce the loss to improve the model accuracy\n",
    "\n",
    "#### III. Loss Function <a id=\"lossFunction\"></a>\n",
    "We use loss functions, which is also known as cost functions to optimize the model during the training. The goal is to minimize the loss function, the lower the better the model is.\n",
    "We utilize cross-entropy as the loss function, it quantifies the difference between the predicted probabilities and actual class labels. The concept and application of cross entropy was drawn from the \"Cross-Entropy Loss\" article by Spot Intelligence [4]. The cross entropy is represented as:\n",
    "\n",
    "\n",
    "$$\n",
    "J = -\\sum_{i=1}^{K} y_i \\log(P(y = i | \\mathbf{X}))\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ J $ is the cross-entropy loss.\n",
    "- $ K $ is the total number of classes.\n",
    "- $ y_i $ is the true label for class $ i $ in one-hot encoded form (1 for the true class and 0 for all others).\n",
    "- $ P(y = i | \\mathbf{X}) $ is the predicted probability that the input $ \\mathbf{X} $ belongs to class $ i $, as output by the softmax function.\n",
    "\n",
    "The average cross-entropy loss is used as a feedback mechanism in the training process, it guides the optimization of the model's weights and biases through gradient descent to minimize this loss across iterations in the training phase for a batch of data points. The average cross-entropy loss used is [4]:\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{i=1}^{K} y_{ni} \\log(P(y = i | \\mathbf{X}_n))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of data points in the batch.\n",
    "- $y_{ni}$ is the true label for class $i$ for the $n$'th data point.\n",
    "- $\\mathbf{X}_n$ is the $n$'th data point.\n",
    "- $P(y = i | \\mathbf{X}_n)$ is the predicted probability for class $i$ for the $n$'th data point.\n",
    "\n",
    "The cross-entropy loss measures how well the probability distribution predicted by the model aligns with the actual distribution of the labels. A lower cross-entropy loss indicates a model that predicts the class labels better. This will help us evaluate and improve our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbqf6I3G6JW2"
   },
   "source": [
    "### 2.1.5 Prediction in Softmax Regression <a id=\"predicitionSoftmaxRegression\"></a>\n",
    "The class with the highest probability is the output class, so we need to compute the predicated class as follows:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = {\\text{argmax}} \\, P(y = i | \\mathbf{X}) {\\text{ for all i }}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_{\\text{pred}} $ is the predicted class label.\n",
    "- $ \\text{argmax} $ is a function that selects the index (class) $ i $ with the maximum probability.\n",
    "- $ P(y = i | \\mathbf{X}) $ is the probability that the input $ \\mathbf{X} $ belongs to class $ i $, as computed by the softmax function.\n",
    "- The $ \\text{argmax} $ function is applied across all classes $ i $ to find the class with the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzOoTwRq6JW2"
   },
   "source": [
    "## 2.2 k-Nearest Neighbors <a id=\"kNN\"></a>\n",
    "k-Nearest Neighbours which is denoted as kNN, is a instance based, Supervised Machine Learning model where the algorithm memorize the training dataset and uses this memorization to make predictions. We are going to explore kNN in the context of classification but it can be used for regression tasks too.\n",
    "\n",
    "Unlike Softmax Regression, kNN is a non parametric which means it makes no assumption about the data distribution. kNN is also a lazy learning algorithm, which means it doesn't have a training phase, Therefore, it makes all the computation on the prediction phase.\n",
    "\n",
    "kNN works in the context of classification by classifying a new data point based on how its neighbors are classified. the kNN Algorithm can be broken down into the following steps:\n",
    "1. Choose the number of neighbours, which is denoted as $ k $.\n",
    "2. Compute the distance of K number of neighbours.\n",
    "3. get $ k $ nearest neighbours based on these distances.\n",
    "4. For classification, the algorithm looks at the classes of these $ k $ neighbours and assigns the class to the new point based on a majority vote.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQNMet0Q6JW2"
   },
   "source": [
    "For the distance metric, we use the euclidean distance to measure the shortest path between two points\n",
    "$$\n",
    "  d_{\\text{euclidean}}(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cl61nN-6JW3"
   },
   "source": [
    "The steps to get $ k $ nearest neighbuor by:\n",
    "1. Compute the Euclidean distance $ d_{\\text{euclidean}}$ between the point we want to classify, which is the testing pointand all the points in the training set.\n",
    "2. Sort the lists of distances in an ascending order. from to each in the training set are then arranged in ascending order\n",
    "3. Pick the top $ k $ points from this sorted list, which is the $ k $ nearest neighbours of the testing point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiV4D3OA6JW3"
   },
   "source": [
    "After identifying the k nearest neighbours of a test point, we then need to determine the most common class among these neighbours which is the majority vote. The argmax selects the class c with the most neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY_LGx5O6JW3"
   },
   "source": [
    "# 3. Methodology <a id=\"methodology\"></a>\n",
    "This section deals with the approach of training and evaluating our models. We start by feature scaling the dataset by normalizing the pixels values of the $ 28 \\times 28 $ images into a range of 0-1 by dividing them by 255.\n",
    "\n",
    "We split the dataset into 60% training, 20% validation and 20% training. The large size of the dataset allows us to allocate significant portions to training without compromising the size of the validation and test size. \n",
    "Finally, We use 3-Fold cross-validation, meaning each data points gets to be in the test set once and in the training set two times. This is suitable for both kNN and Softmax regression models for preventing overfitting since we are not just limited to a specific part of the data.\n",
    "\n",
    "To assess the performance of our models, we utilize the following metrics:\n",
    "- <b>Confusion matrix</b>: Describes the performance of the classification models on a set of test data in which the true values are known. This approach allows us to visualize the accuracy of the model in predicting each class and to identify which classes are most frequently confused with another.\n",
    "- <b>Accuracy</b>: Gives us the proportion of true results (true positives and true negatives) among the total number of cases. High accuracy indicates that the model performs well across all digit classifications.\n",
    "- <b>Precision</b>: Gives us the proportion of positive identification that was actually correct.\n",
    "- <b>Recall</b>: Gives us the proportion of positives that was identified correctly.\n",
    "- <b>F1 score:</b> Gives is the harmonic mean of these two metrics. This metric provides us the overall balance of the model's performance across all digit classes.\n",
    "\n",
    "These performance metrics provide for us a comprehensive picture of the model's performance in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OgC7iBZ6JW3"
   },
   "source": [
    "# 4. Results <a id=\"results\"></a>\n",
    "This section showcases the results derived from the experiments conducted in the notebook using the established methodology. It delves into the training and validation loss of our custom built Softmax Regression model, the confusion matrices of the three models (k-Nearest Neighbors implemented from Scikit-learn, our custom Softmax Regression, and Scikit-learn's Softmax Regression), the performance metrics obtained from our 3-fold cross-validation process and insights into our Softmax Regression model's decision-making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Softmax Regression Loss Function <a id=\"softmaxresults\"></a>\n",
    "The convergence of the loss function in our Softmax Regression model is indication of a well fitting model. As demonstrated in the graph below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0EB8UsO6JW3"
   },
   "source": [
    "![loss function over epochs](images/loss_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygj_GgkL6JW3"
   },
   "source": [
    "the training and validation loss decreasing over epochs and stabilizing, demonstrating the model's ability to learn from the data without overfitting. This indicates that our model is increasing in accuracy in predicting the target classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ge2lbGu6JW3"
   },
   "source": [
    "## 4.2. Confusion Matrices <a id=\"confusionresults\"></a>\n",
    "it's important to note that we relied on the Scikit-learn training of kNN for our analysis, since our custom-built kNN model was not used  due to its high computational time it took.\n",
    "\n",
    "In the comparative analysis of Softmax Regression and kNN models, both have shown proficiency in classifying the MNIST dataset. While kNN boasts higher average metrics, meaning superior performance in recognizing digits, Softmax Regression demonstrates notable consistency between the custom and Sklearn implementations. As demonstrate in the following confusion matrix for each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9XmX_HR6JW3"
   },
   "source": [
    "![](images/confusion_matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEM47WKp6JW3"
   },
   "source": [
    "All three models perform relatively well, with high true positive rates for most digits. However, kNN seems to have slightly more true positives for several classes. Given that our softmax regression and sklearn softmax regression models are based on the same underlying principle (softmax function for multiclass classification), it's reassuring to see consistency and similarity in their performance between each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P5ewv-O6JW3"
   },
   "source": [
    "## 4.3. 3-Fold Cross Validation <a id=\"foldresults\"></a>\n",
    "In our 3-fold cross-validation study, we observed that the kNN Classifier achieved an average accuracy, precision, recall, and F1 score all above 0.97. The Softmax Regression model, while slightly lower in metrics, still achieved around 0.91 across all measures. the Sklearn Softmax Regression model showed a small improvement over our custom implementation, with all metrics approximately at 0.916. These findings are summarized in the following table:\n",
    "\n",
    "| Model                         | Accuracy  | Precision | Recall    | F1 Score  |\n",
    "|-------------------------------|-----------|-----------|-----------|-----------|\n",
    "| Softmax Regression            | 0.9096    | 0.9094    | 0.9096    | 0.9091    |\n",
    "| kNN Classifier                | 0.9709    | 0.9711    | 0.9709    | 0.9709    |\n",
    "| Sklearn Softmax Regression    | 0.9161    | 0.9159    | 0.9161    | 0.9160    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhSE6VZG6JW3"
   },
   "source": [
    "The table shows the comparative effectiveness of each model within the scope of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-1lvilW6JW3"
   },
   "source": [
    "## 4.4. Softmax Regression Decision Process <a id=\"decisionresults\"></a>\n",
    "The heatmap visualization of the Softmax model's weights reveals specific pixel areas pivotal for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30q03ipX6JW4"
   },
   "source": [
    "![heap map digits](images/heatmap_digits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl20qNw66JW4"
   },
   "source": [
    "The plot above shows a heatmap for each image corresponding to one class of digit and the colours represent the strength and direction of the weights for each pixel. Pixels with a positive influence are brightly coloured, while those negatively influencing or deemed less significant appear darker.\n",
    "\n",
    "The model appears to focus on the central regions for digits like $0$, $6$, $8$, and $9$, which are characterized by their central loops or curves. For digits like $1$, $7$, and $4$ the vertical lines are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNGpY6jC6JW4"
   },
   "source": [
    "# 5. Evaluation <a id=\"evaluation\"></a>\n",
    "kNN demonstrates slightly superior performance in terms of overall accuracy and F1 score. Softmax Regression is faster to train than kNN, especially as the dataset grows in size. We saw it with our in house kNN implementation that struggled with computational complexity with a large dataset. Softmax Regression is interpretable compared to kNN, the model coefficients and weights provided us insights into the importance and patterns of different features for each label, which can be valuable for understanding model decisions and for feature engineering.\n",
    "\n",
    "The study faced limitations in the computational demands of our custom kNN with larger datasets, showing the practical constraints of non-parametric models in real-world applications. We also gained insights from Softmax Regression into feature importance and patterns for understanding model decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tzjXLqA6JW4"
   },
   "source": [
    "# 6. Conclusion <a id=\"conclusion\"></a>\n",
    "Our study effectively compared Softmax Regression (Multinomial Logistic Regression) and k-Nearest Neighbors (kNN) within the realm of multi class classification using the MNIST digits dataset. We found that while kNN demonstrated superior accuracy and F1 scores, Softmax Regression offered faster training and interpretability. This underscores the importance of choosing the right model based on requirements, whether it's accuracy in complex classification tasks or efficiency and interpretability for applications where these are priorities. Our findings provide a valuable guide for model selection in multi class classification scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Future Work <a id=\"futurework\"></a>\n",
    "Our study focused on the application of Softmax Regression and k-Nearest Neighbors to the MNIST dataset. Some areas for further exploration could include: \n",
    "- Exploring different datasets such as different images resolutions, colour images, or datasets from other domains would provide insights into adaptability of the models.\n",
    "- Due the computational intensity that we faced with our kNN in high dimensional data. Implementing Principal Component Analysis for dimension reduction could provide some valuable insights such as visualizing decision boundaries of kNN for interpretability.\n",
    "- Comparison between between these traditional machine learning models with deep learning models such as Convolutional Neural Networks in multi-class classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. References <a id=\"references\"></a>\n",
    "\n",
    "[1]: Amarnath, R. and Kumar V, V. Pruning Distorted Images in MNIST Handwritten Digits. arXiv. Available at: https://arxiv.org/abs/2307.14343. [06/01/2024]\n",
    "\n",
    "[2] Wikipedia contributors. Multinomial logistic regression. Wikipedia. Available at: https://en.wikipedia.org/wiki/Multinomial_logistic_regression [06/01/2024].\n",
    "\n",
    "[3] Hastie, T., Tibshirani, R., and Friedman, J. The Elements of Statistical Learning.\n",
    "\n",
    "[4] Spot Intelligence.. Cross-Entropy Loss. Available at: https://spotintelligence.com/2023/09/27/cross-entropy-loss/ [06/01/2024].\n",
    "\n",
    "GeeksforGeeks. ML | One Hot Encoding of datasets in Python. Available at: https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/ [06/01/2024].\n",
    "\n",
    "Wikipedia contributors. Softmax function. Wikipedia. Available at: https://en.wikipedia.org/wiki/Softmax_function [06/01/2024].\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
